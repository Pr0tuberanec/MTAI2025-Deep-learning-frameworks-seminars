{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ypQKMC0QIjA",
        "outputId": "46f1a6e2-432f-4bbc-9e7a-083d1f6fb2ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch                                    2.8.0+cu126\n",
            "torchao                                  0.10.0\n",
            "torchaudio                               2.8.0+cu126\n",
            "torchdata                                0.11.0\n",
            "torchsummary                             1.5.1\n",
            "torchtune                                0.6.1\n",
            "torchvision                              0.23.0+cu126\n"
          ]
        }
      ],
      "source": [
        "!pip list | grep torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhEMHX1V4IwM"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoMW3Ruptax1"
      },
      "source": [
        "*   https://pytorch.org/blog/quantization-in-practice/\n",
        "*   https://pytorch.org/docs/stable/quantization.html\n",
        "*   https://pytorch.org/docs/stable/quantization-support.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbqyoRrl7A0y"
      },
      "source": [
        "### Mapping function and Quantization Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbgzwD7M1KW8"
      },
      "outputs": [],
      "source": [
        "# r - float tensor, r' - int tensor\n",
        "# S [scaling factor] = (beta - alpha) / (beta_q - alpha_q)\n",
        "# Z [zero point] =  -(alpha / S - alpha_q)\n",
        "\n",
        "def quantize(float_tensor, scale, z):\n",
        "  # Q(r) = round(r/S + Z)\n",
        "  return torch.round(float_tensor / scale + z)\n",
        "\n",
        "def dequantize(int_tensor, scale, z):\n",
        "  # r' = (Q(r) - Z) * S\n",
        "  return (int_tensor - z) * scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBAf6XZi1KXs",
        "outputId": "ff1346c3-d194-479b-f8e4-44585aa0cfbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 3.2171e-01,  3.0418e+00, -1.4741e-01, -1.0195e+00],\n",
            "        [-1.4022e+00,  6.9944e-01,  2.0892e-03,  1.9233e+00],\n",
            "        [-4.3304e-01,  1.6073e+00, -3.8651e-02, -8.2203e-01]])\n"
          ]
        }
      ],
      "source": [
        "from torch.ao.quantization.observer import MinMaxObserver, MovingAverageMinMaxObserver, HistogramObserver\n",
        "\n",
        "C, L = 3, 4\n",
        "normal = torch.distributions.normal.Normal(0,1)\n",
        "inputs = normal.sample((C, L))\n",
        "print(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPanD_eQ4TZl",
        "outputId": "cca01292-d01e-424f-8c33-6f9c0a80b774"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MinMaxObserver (tensor([0.0174]), tensor([80], dtype=torch.int32))\n",
            "MovingAverageMinMaxObserver (tensor([0.0174]), tensor([80], dtype=torch.int32))\n",
            "HistogramObserver (tensor([0.0174]), tensor([80], dtype=torch.int32))\n"
          ]
        }
      ],
      "source": [
        "observers = [MinMaxObserver(), MovingAverageMinMaxObserver(), HistogramObserver()]\n",
        "for obs in observers:\n",
        "  obs(inputs)\n",
        "  print(obs.__class__.__name__, obs.calculate_qparams())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZf3OgNQ49Aj",
        "outputId": "1c91a1cb-3ce2-4bd8-ae41-75d1ffb13d3b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.0080, 0.0080, 0.0080, 0.0087],\n",
              "        [0.0080, 0.0024, 0.0021, 0.0063],\n",
              "        [0.0026, 0.0040, 0.0038, 0.0029]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scale, z = observers[0].calculate_qparams()\n",
        "reconstruction_error = torch.abs(dequantize(quantize(inputs, scale, z), scale, z) - inputs)\n",
        "reconstruction_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb5qUkHP8sUE"
      },
      "source": [
        "### Affine and Symmetric Quantization Schemes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8cv_y6f8tT3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def get_symmetric_range(x):\n",
        "  beta = torch.max(x.max(), x.min().abs())\n",
        "  return -beta.item(), beta.item()\n",
        "\n",
        "def get_affine_range(x):\n",
        "  return x.min().item(), x.max().item()\n",
        "\n",
        "def plot(plt, data, scheme):\n",
        "  boundaries = get_affine_range(data) if scheme == 'affine' else get_symmetric_range(data)\n",
        "  a, _, _ = plt.hist(data, density=True, bins=100)\n",
        "  ymin, ymax = np.quantile(a[a>0], [0.25, 0.95])\n",
        "  plt.vlines(x=boundaries, ls='--', colors='purple', ymin=ymin, ymax=ymax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NTMv0kcB8128",
        "outputId": "6ae63c10-0d68-4ae2-8865-c0231cd93eef"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "act =  torch.distributions.pareto.Pareto(1, 10).sample((1, 1024))\n",
        "weights = torch.distributions.normal.Normal(0, 0.12).sample((3, 64, 7, 7)).flatten()\n",
        "\n",
        "fig, axs = plt.subplots(2,2)\n",
        "plot(axs[0, 0], act, 'affine')\n",
        "axs[0, 0].set_title(\"Activation, Affine-Quantized\")\n",
        "\n",
        "plot(axs[0, 1], act, 'symmetric')\n",
        "axs[0, 1].set_title(\"Activation, Symmetric-Quantized\")\n",
        "\n",
        "plot(axs[1, 0], weights, 'affine')\n",
        "axs[1, 0].set_title(\"Weights, Affine-Quantized\")\n",
        "\n",
        "plot(axs[1, 1], weights, 'symmetric')\n",
        "axs[1, 1].set_title(\"Weights, Symmetric-Quantized\")\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bej5FWvR-UDK",
        "outputId": "bcdc908e-5ac2-4d2c-c9a7-c7a93b4f1aae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Qscheme: torch.per_tensor_affine | (tensor([0.0111]), tensor([163], dtype=torch.int32))\n",
            "Qscheme: torch.per_tensor_symmetric | (tensor([0.0142]), tensor([128]))\n"
          ]
        }
      ],
      "source": [
        "for qscheme in [torch.per_tensor_affine, torch.per_tensor_symmetric]:\n",
        "  obs = MovingAverageMinMaxObserver(qscheme=qscheme)\n",
        "  obs(inputs)\n",
        "  print(f\"Qscheme: {qscheme} | {obs.calculate_qparams()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc4Zd5M99fTo"
      },
      "source": [
        "### Per-Tensor and Per-Channel Quantization Schemes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Dltyo4q9l-4",
        "outputId": "10a08eda-a0cf-41f4-f1b5-d392cd4c3428"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor([0.0094, 0.0045, 0.0111]), tensor([186, 144, 163], dtype=torch.int32))\n"
          ]
        }
      ],
      "source": [
        "from torch.ao.quantization.observer import MovingAveragePerChannelMinMaxObserver\n",
        "\n",
        "obs = MovingAveragePerChannelMinMaxObserver(ch_axis = 0)  # calculate qparams for all `C` channels separately\n",
        "obs(inputs)\n",
        "print(obs.calculate_qparams())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOd8W97_AAMW"
      },
      "source": [
        "For weights quantization, symmetric-per-channel quantization provides better accuracies; per-tensor quantization performs poorly, possibly due to high variance in conv weights across channels from batchnorm folding.\n",
        "[https://arxiv.org/abs/2004.09602]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvWTOGgv9gmI"
      },
      "source": [
        "https://github.com/pytorch/pytorch/blob/748d9d24940cd17938df963456c90fa1a13f3932/torch/ao/quantization/observer.py#L258"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiBMYYJMBC0A"
      },
      "source": [
        "### Backend Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OgF7U86BDw7"
      },
      "outputs": [],
      "source": [
        "backend = 'fbgemm' #'fbgemm' if x86 else 'qnnpack' intel - oneDNN\n",
        "qconfig = torch.ao.quantization.get_default_qconfig(backend)\n",
        "torch.backends.quantized.engine = backend"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atxxYA28Bcvz"
      },
      "source": [
        "GPUs - via TensorRT and cuDNN\n",
        "https://pytorch.org/docs/stable/quantization.html#note-for-native-cpu-backends"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jk-uxMCWBpRl"
      },
      "source": [
        "### QConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3xygBpkBqKW",
        "outputId": "14dcad9c-e6f9-4c3d-ed18-6965290fc05c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, qscheme=torch.per_tensor_affine){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, qscheme=torch.qint8){})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "my_qconfig = torch.ao.quantization.QConfig(\n",
        "  activation=MovingAverageMinMaxObserver.with_args(qscheme=torch.per_tensor_affine),\n",
        "  weight=MovingAveragePerChannelMinMaxObserver.with_args(qscheme=torch.qint8))\n",
        "\n",
        "my_qconfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvsq4t68Kwj7",
        "outputId": "497c7aef-f6d4-464c-ed63-a9a24b63059a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.per_channel_affine"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mh00mEaV4wy"
      },
      "source": [
        "### Eager Mode v/s FX Graph Mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLkEva02V_Co"
      },
      "source": [
        "*    Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals.\n",
        "*    FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently its a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWjnQot3Wzg-"
      },
      "source": [
        "FX Graph Mode automatically fuses eligible modules, inserts Quant/DeQuant stubs, calibrates the model and returns a quantized module - all in two method calls - but only for networks that are symbolic traceable. The examples below contain the calls using Eager Mode and FX Graph Mode for comparison.\n",
        "https://pytorch.org/docs/stable/fx.html#torch.fx.symbolic_trace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "el4bJKgIV6y7"
      },
      "outputs": [],
      "source": [
        "def f(a, b):\n",
        "    if b == True:\n",
        "        return a\n",
        "    else:\n",
        "        return a * 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "PkVCexZefVIX",
        "outputId": "c726d8dc-1a30-4987-d196-2755639d20f2"
      },
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "b has been specialized to have value False but got another value",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1040574657.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcrete_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py\u001b[0m in \u001b[0;36mcall_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcall_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_wrapped\u001b[0m  \u001b[0;31m# type: ignore[method-assign]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa: B904\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<eval_with_key>.4 from <eval_with_key>.3 from <eval_with_key>.2 from /tmp/ipython-input-2855332725.py:1 in f:4 in forward:4 in forward\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, a, b_1)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0meq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb_1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mb_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0m_assert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b has been specialized to have value False but got another value'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0meq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_assert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmul\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmul\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m_assert\u001b[0;34m(condition, message)\u001b[0m\n\u001b[1;32m   2172\u001b[0m             \u001b[0m_assert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2173\u001b[0m         )\n\u001b[0;32m-> 2174\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: b has been specialized to have value False but got another value"
          ]
        }
      ],
      "source": [
        "f = torch.fx.symbolic_trace(f, concrete_args={'b': False})\n",
        "assert f(3, True) == 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Temh_oTBfjE8"
      },
      "source": [
        "https://pytorch.org/docs/stable/fx.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfxsmoBtXOgb"
      },
      "source": [
        "### Post-Training Dynamic/Weight-only Quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APgH-cqYdx6O"
      },
      "source": [
        "Here the models weights are pre-quantized; the activations are quantized on-the-fly (dynamic) during inference. The simplest of all approaches, it has a one line API call in torch.quantization.quantize_dynamic. Currently only Linear and Recurrent (LSTM, GRU, RNN) layers are supported for dynamic quantization\n",
        "\n",
        "\n",
        "\n",
        "*   Can result in higher accuracies since the clipping range is exactly calibrated for each input\n",
        "*   Calibrating and quantizing the activations at each layer during runtime can add to the compute overhead\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqIHv8efWt2l",
        "outputId": "2e8d79e3-ef19-4da1-eb7e-fc2eda357a0e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(2, 64, kernel_size=(8,), stride=(1, 1))\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=16, out_features=10, bias=True)\n",
              "  (3): LSTM(10, 10)\n",
              ")"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torch import nn\n",
        "\n",
        "# toy model\n",
        "m = nn.Sequential(\n",
        "  nn.Conv2d(2, 64, (8,)),\n",
        "  nn.ReLU(),\n",
        "  nn.Linear(16, 10),\n",
        "  nn.LSTM(10, 10))\n",
        "\n",
        "m.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jhhTCrvXYv4",
        "outputId": "fc99d28f-0639-4782-b741-86dfdb70138a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2181302342.py:4: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model_quantized = quantize_dynamic(\n"
          ]
        }
      ],
      "source": [
        "## EAGER MODE\n",
        "from torch.ao.quantization import quantize_dynamic\n",
        "\n",
        "model_quantized = quantize_dynamic(\n",
        "    model=m, qconfig_spec={nn.LSTM, nn.Linear}, dtype=torch.qint8, inplace=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PC5aJPPgXaO_",
        "outputId": "bad84414-ae8e-4cec-ef68-c453fd0254b4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-15622409.py:6: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model_prepared = quantize_fx.prepare_fx(m, qconfig_dict, example_inputs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  prepared = prepare(\n",
            "/tmp/ipython-input-15622409.py:7: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model_quantized = quantize_fx.convert_fx(model_prepared)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/nn/quantized/reference/modules/rnn.py:461: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  torch.tensor(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/nn/quantized/reference/modules/rnn.py:467: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  torch.tensor(\n"
          ]
        }
      ],
      "source": [
        "## FX MODE\n",
        "from torch.ao.quantization import quantize_fx\n",
        "\n",
        "example_inputs = (torch.randn(1, 3, 224, 224),)\n",
        "qconfig_dict = {\"\": torch.ao.quantization.default_dynamic_qconfig}  # An empty key denotes the default applied to all modules\n",
        "model_prepared = quantize_fx.prepare_fx(m, qconfig_dict, example_inputs)\n",
        "model_quantized = quantize_fx.convert_fx(model_prepared)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lwvpek4HMxer",
        "outputId": "d3795509-45bc-4926-9757-2400ad784fc6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.quint8, quant_min=0, quant_max=255, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.ao.quantization.default_dynamic_qconfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_Dd8gxLaNmy"
      },
      "source": [
        "### Post-Training Static Quantization (PTQ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCZcZdAIdTzg"
      },
      "source": [
        "PTQ also pre-quantizes model weights but instead of calibrating activations on-the-fly, the clipping range is pre-calibrated and fixed (static) using validation data. Activations stay in quantized precision between operations during inference. About 100 mini-batches of representative data are sufficient to calibrate the observers\n",
        "\n",
        "*   Static quantization has faster inference than dynamic quantization because it eliminates the float<->int conversion costs between layers\n",
        "*   Static quantized models may need regular re-calibration to stay robust against distribution-drift\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3v3vGj4WaQYV"
      },
      "outputs": [],
      "source": [
        "# Static quantization of a model consists of the following steps:\n",
        "\n",
        "#     Fuse modules\n",
        "#     Insert Quant/DeQuant Stubs\n",
        "#     Prepare the fused module (insert observers before and after layers)\n",
        "#     Calibrate the prepared module (pass it representative data)\n",
        "#     Convert the calibrated module (replace with quantized version)\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import copy\n",
        "\n",
        "backend = \"fbgemm\"  # running on a x86 CPU. Use \"qnnpack\" if running on ARM.\n",
        "\n",
        "model = nn.Sequential(\n",
        "     nn.Conv2d(2, 64, 3),\n",
        "     nn.ReLU(),\n",
        "     nn.Conv2d(64, 128, 3),\n",
        "     nn.ReLU()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "UaZu_-p7OMgf",
        "outputId": "97e7167b-3973-4b5d-c119-29e38d860d99"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'Parameter' object is not callable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3628335316.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'Parameter' object is not callable"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgNoxdlneyuM",
        "outputId": "bb10ac74-9428-437e-f867-36097e1a4860"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-213743822.py:18: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  torch.ao.quantization.prepare(m, inplace=True)\n",
            "/tmp/ipython-input-213743822.py:29: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  torch.ao.quantization.convert(m, inplace=True)\n"
          ]
        }
      ],
      "source": [
        "## EAGER MODE\n",
        "m = copy.deepcopy(model)\n",
        "m.eval()\n",
        "\n",
        "\"\"\"Fuse\n",
        "- Inplace fusion replaces the first module in the sequence with the fused module, and the rest with identity modules\n",
        "\"\"\"\n",
        "torch.ao.quantization.fuse_modules(m, ['0','1'], inplace=True) # fuse first Conv-ReLU pair\n",
        "torch.ao.quantization.fuse_modules(m, ['2','3'], inplace=True) # fuse second Conv-ReLU pair\n",
        "\n",
        "\"\"\"Insert stubs\"\"\"\n",
        "m = nn.Sequential(torch.ao.quantization.QuantStub(),\n",
        "                  *m,\n",
        "                  torch.ao.quantization.DeQuantStub())\n",
        "\n",
        "\"\"\"Prepare\"\"\"\n",
        "m.qconfig = torch.ao.quantization.get_default_qconfig(backend)\n",
        "torch.ao.quantization.prepare(m, inplace=True)\n",
        "\n",
        "\"\"\"Calibrate\n",
        "- This example uses random data for convenience. Use representative (validation) data instead.\n",
        "\"\"\"\n",
        "with torch.inference_mode():\n",
        "  for _ in range(10):\n",
        "    x = torch.rand(1, 2, 28, 28)\n",
        "    m(x)\n",
        "\n",
        "\"\"\"Convert\"\"\"\n",
        "torch.ao.quantization.convert(m, inplace=True)\n",
        "\n",
        "\"\"\"Check\"\"\"\n",
        "print(m[1].weight().element_size()) # 1 byte instead of 4 bytes for FP32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmyPkO1Bi-Tw",
        "outputId": "dd350a35-e4c4-46da-950a-536bfe096324"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[[-0.0933, -0.0842, -0.0275],\n",
              "          [ 0.0439,  0.0714, -0.0146],\n",
              "          [-0.2342, -0.1501,  0.1592]],\n",
              "\n",
              "         [[ 0.0037, -0.0073,  0.0384],\n",
              "          [ 0.0860, -0.1574,  0.1043],\n",
              "          [ 0.0622,  0.0842,  0.0238]]],\n",
              "\n",
              "\n",
              "        [[[ 0.1001, -0.1001, -0.1244],\n",
              "          [ 0.0873, -0.0901, -0.1545],\n",
              "          [-0.1230, -0.1831, -0.1631]],\n",
              "\n",
              "         [[-0.0429,  0.0229, -0.1302],\n",
              "          [-0.0501,  0.0300,  0.0772],\n",
              "          [-0.1530,  0.1001,  0.0529]]],\n",
              "\n",
              "\n",
              "        [[[ 0.0224,  0.1222,  0.0671],\n",
              "          [ 0.1136,  0.0826, -0.1584],\n",
              "          [ 0.1205,  0.1859, -0.1136]],\n",
              "\n",
              "         [[-0.2204,  0.2135,  0.1308],\n",
              "          [ 0.2014,  0.1532,  0.0465],\n",
              "          [-0.2066, -0.0207, -0.1153]]],\n",
              "\n",
              "\n",
              "        ...,\n",
              "\n",
              "\n",
              "        [[[-0.1481, -0.1252,  0.1816],\n",
              "          [ 0.1869,  0.1728, -0.0599],\n",
              "          [ 0.1199, -0.2169, -0.0176]],\n",
              "\n",
              "         [[-0.0194,  0.1358, -0.0071],\n",
              "          [-0.0212, -0.0282,  0.0000],\n",
              "          [-0.0952, -0.0758, -0.2257]]],\n",
              "\n",
              "\n",
              "        [[[ 0.0545,  0.0514, -0.1262],\n",
              "          [ 0.1573, -0.0498, -0.0016],\n",
              "          [ 0.0717, -0.0265, -0.1854]],\n",
              "\n",
              "         [[-0.1682,  0.0343,  0.0093],\n",
              "          [-0.1651, -0.1340, -0.1090],\n",
              "          [-0.0016,  0.1978,  0.1978]]],\n",
              "\n",
              "\n",
              "        [[[ 0.1866, -0.0879, -0.1956],\n",
              "          [-0.1471,  0.2279,  0.0574],\n",
              "          [-0.1112, -0.1812, -0.2189]],\n",
              "\n",
              "         [[-0.0269, -0.0054,  0.1292],\n",
              "          [-0.0126,  0.1453, -0.0144],\n",
              "          [ 0.0449,  0.0431, -0.0969]]]], size=(64, 2, 3, 3),\n",
              "       dtype=torch.qint8, quantization_scheme=torch.per_channel_affine,\n",
              "       scale=tensor([0.0018, 0.0014, 0.0017, 0.0018, 0.0017, 0.0017, 0.0018, 0.0018, 0.0015,\n",
              "        0.0015, 0.0018, 0.0018, 0.0016, 0.0015, 0.0018, 0.0018, 0.0018, 0.0018,\n",
              "        0.0017, 0.0018, 0.0017, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018,\n",
              "        0.0017, 0.0017, 0.0018, 0.0015, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018,\n",
              "        0.0018, 0.0016, 0.0018, 0.0015, 0.0017, 0.0017, 0.0018, 0.0018, 0.0018,\n",
              "        0.0018, 0.0018, 0.0017, 0.0017, 0.0018, 0.0018, 0.0018, 0.0017, 0.0018,\n",
              "        0.0014, 0.0018, 0.0017, 0.0014, 0.0018, 0.0018, 0.0017, 0.0018, 0.0016,\n",
              "        0.0018], dtype=torch.float64),\n",
              "       zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              "       axis=0)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "m[1].weight()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8a-ihuEe13L",
        "outputId": "3e0b71df-9e29-4186-8c88-0a06701c85d6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-750729956.py:10: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model_prepared = quantize_fx.prepare_fx(m, qconfig_dict, example_inputs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  prepared = prepare(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-750729956.py:19: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model_quantized = quantize_fx.convert_fx(model_prepared)\n"
          ]
        }
      ],
      "source": [
        "## FX GRAPH\n",
        "from torch.ao.quantization import quantize_fx\n",
        "\n",
        "m = copy.deepcopy(model)\n",
        "m.eval()\n",
        "qconfig_dict = {\"\": torch.ao.quantization.get_default_qconfig(backend)}\n",
        "\n",
        "# Prepare\n",
        "example_inputs = (torch.randn(1, 3, 224, 224),)\n",
        "model_prepared = quantize_fx.prepare_fx(m, qconfig_dict, example_inputs)\n",
        "\n",
        "# Calibrate - Use representative (validation) data.\n",
        "with torch.inference_mode():\n",
        "  for _ in range(10):\n",
        "    x = torch.rand(1, 2, 28, 28)\n",
        "    model_prepared(x)\n",
        "\n",
        "# quantize\n",
        "model_quantized = quantize_fx.convert_fx(model_prepared)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT-qggJdacLr"
      },
      "source": [
        "### SENSITIVITY ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tD294D_Tac3K",
        "outputId": "f05c158c-b2fe-48a9-a6f6-ac6f0d7f7bac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Only quantizing layer:  \n",
            "Only quantizing layer:  0\n",
            "Only quantizing layer:  1\n",
            "Only quantizing layer:  2\n",
            "Only quantizing layer:  3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1147990886.py:11: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model_prepared = quantize_fx.prepare_fx(model, qconfig_dict, example_inputs)\n",
            "/tmp/ipython-input-1147990886.py:13: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model_quantized = quantize_fx.convert_fx(model_prepared)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:1343: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# ONE-AT-A-TIME SENSITIVITY ANALYSIS\n",
        "\n",
        "for quantized_layer, _ in model.named_modules():\n",
        "  print(\"Only quantizing layer: \", quantized_layer)\n",
        "\n",
        "  # The module_name key allows module-specific qconfigs.\n",
        "  qconfig_dict = {\"\": None,\n",
        "  \"module_name\":[(quantized_layer, torch.quantization.get_default_qconfig(backend))]}\n",
        "\n",
        "  example_inputs = (torch.randn(1, 3, 224, 224),)\n",
        "  model_prepared = quantize_fx.prepare_fx(model, qconfig_dict, example_inputs)\n",
        "  # calibrate\n",
        "  model_quantized = quantize_fx.convert_fx(model_prepared)\n",
        "  # evaluate(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dXAIoKwbCFl"
      },
      "source": [
        " Numeric Suite - https://pytorch.org/tutorials/prototype/numeric_suite_tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7FfZtQdbV90"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import torch.ao.quantization\n",
        "import torch.ao.ns._numeric_suite as ns\n",
        "from torch.ao.quantization import (\n",
        "    default_eval_fn,\n",
        "    default_qconfig,\n",
        "    quantize,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jm-ESg6obWWp"
      },
      "outputs": [],
      "source": [
        "float_model = torchvision.models.quantization.resnet18(pretrained=True, quantize=False)\n",
        "float_model.to('cpu')\n",
        "float_model.eval()\n",
        "float_model.fuse_model()\n",
        "float_model.qconfig = torch.quantization.default_qconfig\n",
        "img_data = (torch.rand(2, 3, 10, 10, dtype=torch.float), torch.randint(0, 1, (2,), dtype=torch.long))\n",
        "qmodel = quantize(float_model, default_eval_fn, [[img_data]], inplace=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKZEOpfPbMNu",
        "outputId": "4c74f046-1594-4f5b-cfd7-38b51f329a1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "conv1.weight tensor(31.6638)\n",
            "layer1.0.conv1.weight tensor(30.6450)\n",
            "layer1.0.conv2.weight tensor(31.1528)\n",
            "layer1.1.conv1.weight tensor(32.1438)\n",
            "layer1.1.conv2.weight tensor(31.2477)\n",
            "layer2.0.conv1.weight tensor(30.9890)\n",
            "layer2.0.conv2.weight tensor(28.8233)\n",
            "layer2.0.downsample.0.weight tensor(31.5558)\n",
            "layer2.1.conv1.weight tensor(30.7668)\n",
            "layer2.1.conv2.weight tensor(28.4516)\n",
            "layer3.0.conv1.weight tensor(30.9247)\n",
            "layer3.0.conv2.weight tensor(26.6841)\n",
            "layer3.0.downsample.0.weight tensor(28.7825)\n",
            "layer3.1.conv1.weight tensor(28.9707)\n",
            "layer3.1.conv2.weight tensor(25.6784)\n",
            "layer4.0.conv1.weight tensor(26.8495)\n",
            "layer4.0.conv2.weight tensor(25.8394)\n",
            "layer4.0.downsample.0.weight tensor(28.6355)\n",
            "layer4.1.conv1.weight tensor(26.8758)\n",
            "layer4.1.conv2.weight tensor(28.4319)\n",
            "fc._packed_params._packed_params tensor(32.6505)\n",
            "---\n",
            "conv1.stats tensor(37.4411, grad_fn=<MulBackward0>)\n",
            "layer1.0.conv1.stats tensor(29.6032, grad_fn=<MulBackward0>)\n",
            "layer1.0.conv2.stats tensor(28.3895, grad_fn=<MulBackward0>)\n",
            "layer1.0.add_relu.stats tensor(31.8408, grad_fn=<MulBackward0>)\n",
            "layer1.1.conv1.stats tensor(29.1966, grad_fn=<MulBackward0>)\n",
            "layer1.1.conv2.stats tensor(25.3061, grad_fn=<MulBackward0>)\n",
            "layer1.1.add_relu.stats tensor(28.9989, grad_fn=<MulBackward0>)\n",
            "layer2.0.conv1.stats tensor(26.2547, grad_fn=<MulBackward0>)\n",
            "layer2.0.conv2.stats tensor(26.3159, grad_fn=<MulBackward0>)\n",
            "layer2.0.downsample.0.stats tensor(21.8628, grad_fn=<MulBackward0>)\n",
            "layer2.0.add_relu.stats tensor(25.7588, grad_fn=<MulBackward0>)\n",
            "layer2.1.conv1.stats tensor(25.3852, grad_fn=<MulBackward0>)\n",
            "layer2.1.conv2.stats tensor(24.6284, grad_fn=<MulBackward0>)\n",
            "layer2.1.add_relu.stats tensor(25.7878, grad_fn=<MulBackward0>)\n",
            "layer3.0.conv1.stats tensor(26.6457, grad_fn=<MulBackward0>)\n",
            "layer3.0.conv2.stats tensor(26.6580, grad_fn=<MulBackward0>)\n",
            "layer3.0.downsample.0.stats tensor(24.8511, grad_fn=<MulBackward0>)\n",
            "layer3.0.add_relu.stats tensor(24.7550, grad_fn=<MulBackward0>)\n",
            "layer3.1.conv1.stats tensor(29.9616, grad_fn=<MulBackward0>)\n",
            "layer3.1.conv2.stats tensor(25.6695, grad_fn=<MulBackward0>)\n",
            "layer3.1.add_relu.stats tensor(25.3829, grad_fn=<MulBackward0>)\n",
            "layer4.0.conv1.stats tensor(27.0312, grad_fn=<MulBackward0>)\n",
            "layer4.0.conv2.stats tensor(27.1815, grad_fn=<MulBackward0>)\n",
            "layer4.0.downsample.0.stats tensor(22.7665, grad_fn=<MulBackward0>)\n",
            "layer4.0.add_relu.stats tensor(21.4375, grad_fn=<MulBackward0>)\n",
            "layer4.1.conv1.stats tensor(27.0220, grad_fn=<MulBackward0>)\n",
            "layer4.1.conv2.stats tensor(18.7704, grad_fn=<MulBackward0>)\n",
            "layer4.1.add_relu.stats tensor(19.1204, grad_fn=<MulBackward0>)\n",
            "fc.stats tensor(21.6713, grad_fn=<MulBackward0>)\n",
            "quant.stats tensor(48.0328)\n"
          ]
        }
      ],
      "source": [
        "def compute_error(x, y):\n",
        "    Ps = torch.norm(x)\n",
        "    Pn = torch.norm(x - y)\n",
        "    return 20 * torch.log10(Ps/Pn)\n",
        "\n",
        "wt_compare_dict = ns.compare_weights(float_model.state_dict(), qmodel.state_dict())\n",
        "for key in wt_compare_dict:\n",
        "    print(key, compute_error(wt_compare_dict[key]['float'], wt_compare_dict[key]['quantized'].dequantize()))\n",
        "\n",
        "print(\"---\")\n",
        "\n",
        "act_compare_dict = ns.compare_model_outputs(float_model, qmodel, img_data[0])\n",
        "for key in act_compare_dict:\n",
        "    print(key, compute_error(act_compare_dict[key]['float'][0], act_compare_dict[key]['quantized'][0].dequantize()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "eYUaWoAyeL7a",
        "outputId": "d6706d68-1d63-42f4-ee3f-05a519ad46b9"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "f = wt_compare_dict['conv1.weight']['float'].flatten()\n",
        "plt.hist(f, bins = 100)\n",
        "\n",
        "q = wt_compare_dict['conv1.weight']['quantized'].flatten().dequantize()\n",
        "plt.hist(q, bins = 100)\n",
        "plt.title(\"Quantized model weights of conv1\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YQ7S0iIb8cz"
      },
      "source": [
        "### RECOMMENDATIONS FOR YOUR WORKFLOW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-cxT3iwb9jU"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1ij7GElwiKFNmJ221aKpkDRwMj5i-2DLh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBbyE61acqr6"
      },
      "source": [
        "\n",
        "\n",
        "*   Large (10M+ parameters) models are more robust to quantization error\n",
        "*   Quantizing a model from a FP32 checkpoint provides better accuracy than training an INT8 model from scratch\n",
        "*   Dynamic Quantization is an easy first step, especially if your model has many Linear or Recurrent layers\n",
        "*   Use symmetric-per-channel quantization with MinMax observers for quantizing weights. Use affine-per-tensor quantization with MovingAverageMinMax observers for quantizing activations\n",
        "*   Use metrics like SQNR to identify which layers are most suscpetible to quantization error. Turn off quantization on these layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZkMfDNCp9f1"
      },
      "source": [
        "Eager mode https://pytorch.org/docs/stable/quantization.html#eager-mode-quantization\n",
        "\n",
        "\n",
        "\n",
        "*   https://pytorch.org/docs/stable/quantization.html#post-training-dynamic-quantization\n",
        "*   https://pytorch.org/docs/stable/quantization.html#post-training-static-quantization\n",
        "\n",
        "\n",
        "FX Graph https://pytorch.org/docs/stable/quantization.html#prototype-fx-graph-mode-quantization\n",
        "\n",
        "\n",
        "\n",
        "*   https://pytorch.org/tutorials/prototype/fx_graph_mode_ptq_static.html\n",
        "*   https://pytorch.org/tutorials/prototype/fx_graph_mode_ptq_dynamic.html\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "za9ziysYh-7V"
      },
      "source": [
        "## New forkflow with torchao\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHc-7SBJiWid",
        "outputId": "743889a9-28f3-4a2b-a2e0-027d3abed8f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchao in /usr/local/lib/python3.12/dist-packages (0.10.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchao"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LP_lwV2iGXF"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch\n",
        "\n",
        "class ToyLinearModel(torch.nn.Module):\n",
        "    def __init__(self, m: int, n: int, k: int):\n",
        "        super().__init__()\n",
        "        self.linear1 = torch.nn.Linear(m, n, bias=False)\n",
        "        self.linear2 = torch.nn.Linear(n, k, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "model = ToyLinearModel(1024, 1024, 1024).eval()\n",
        "\n",
        "# Optional: compile model for faster inference and generation\n",
        "model = torch.compile(model, mode=\"max-autotune\", fullgraph=True)\n",
        "model_f32 = copy.deepcopy(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cYxfwkt-xrX"
      },
      "outputs": [],
      "source": [
        "from torchao.quantization import Int4DynamicActivationInt4WeightConfig, quantize_\n",
        "quantize_(model, Int4DynamicActivationInt4WeightConfig())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbGjMaxGj20y",
        "outputId": "22e92824-8902-400b-c5d9-10366d82920f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W1027 10:36:26.360000 1094 torch/utils/cpp_extension.py:118] [0/0] No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "f32 mean time: 0.123 ms\n",
            "int4 mean time: 0.162 ms\n",
            "speedup: 0.8x\n"
          ]
        }
      ],
      "source": [
        "from torchao.utils import (\n",
        "    benchmark_model,\n",
        "    unwrap_tensor_subclass,\n",
        ")\n",
        "\n",
        "num_runs = 100\n",
        "torch._dynamo.reset()\n",
        "example_inputs = (torch.randn(1, 1024, dtype=torch.float32),)\n",
        "f32_time = benchmark_model(model_f32, num_runs, example_inputs)\n",
        "int4_time = benchmark_model(model, num_runs, example_inputs)\n",
        "\n",
        "print(\"f32 mean time: %0.3f ms\" % f32_time)\n",
        "print(\"int4 mean time: %0.3f ms\" % int4_time)\n",
        "print(\"speedup: %0.1fx\" % (f32_time / int4_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWDUskVC-H1z",
        "outputId": "9656c9bc-b85f-49e4-9258-7c92b21848de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "int4 model size: 1.01 MB\n",
            "f32 model size: 8.00 MB\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "torch.save(model, \"/tmp/int4_model.pt\")\n",
        "torch.save(model_f32, \"/tmp/f32_model.pt\")\n",
        "int4_model_size_mb = os.path.getsize(\"/tmp/int4_model.pt\") / 1024 / 1024\n",
        "f32_model_size_mb = os.path.getsize(\"/tmp/f32_model.pt\") / 1024 / 1024\n",
        "\n",
        "print(\"int4 model size: %.2f MB\" % int4_model_size_mb)\n",
        "\n",
        "print(\"f32 model size: %.2f MB\" % f32_model_size_mb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI8kSIybQcEQ"
      },
      "source": [
        "### Neural Network Compression Framework (NNCF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7q1Gh6YQfhu",
        "outputId": "29be0508-a8aa-47b6-fb70-b4c415adb0bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'nncf'...\n",
            "remote: Enumerating objects: 47284, done.\u001b[K\n",
            "remote: Counting objects: 100% (6170/6170), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2248/2248), done.\u001b[K\n",
            "remote: Total 47284 (delta 3294), reused 5398 (delta 2909), pack-reused 41114\u001b[K\n",
            "Receiving objects: 100% (47284/47284), 39.55 MiB | 26.35 MiB/s, done.\n",
            "Resolving deltas: 100% (31350/31350), done.\n",
            "Filtering content: 100% (142/142), 32.00 MiB | 18.07 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/openvinotoolkit/nncf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7inenttGuOti",
        "outputId": "d08f9842-921d-4e6f-ec83-5a0373e6553f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/nncf\n",
            "Processing /content/nncf\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pymoo@ git+https://github.com/anyoptimization/pymoo.git@695cb26923903f872c7256a9013609769f3cc2bd (from nncf==2.5.0.dev0+9c30388f)\n",
            "  Cloning https://github.com/anyoptimization/pymoo.git (to revision 695cb26923903f872c7256a9013609769f3cc2bd) to /tmp/pip-install-bhxstqg6/pymoo_e66377a95ce74813841ab1a8bb972ab7\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/anyoptimization/pymoo.git /tmp/pip-install-bhxstqg6/pymoo_e66377a95ce74813841ab1a8bb972ab7\n",
            "  Running command git rev-parse -q --verify 'sha^695cb26923903f872c7256a9013609769f3cc2bd'\n",
            "  Running command git fetch -q https://github.com/anyoptimization/pymoo.git 695cb26923903f872c7256a9013609769f3cc2bd\n",
            "  Running command git checkout -q 695cb26923903f872c7256a9013609769f3cc2bd\n",
            "  Resolved https://github.com/anyoptimization/pymoo.git to commit 695cb26923903f872c7256a9013609769f3cc2bd\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: jsonschema>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from nncf==2.5.0.dev0+9c30388f) (4.19.0)\n",
            "Collecting jstyleson>=0.0.2 (from nncf==2.5.0.dev0+9c30388f)\n",
            "  Downloading jstyleson-0.0.2.tar.gz (2.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: natsort>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from nncf==2.5.0.dev0+9c30388f) (8.4.0)\n",
            "Collecting networkx<=2.8.2,>=2.6 (from nncf==2.5.0.dev0+9c30388f)\n",
            "  Downloading networkx-2.8.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ninja<1.11,>=1.10.0.post2 (from nncf==2.5.0.dev0+9c30388f)\n",
            "  Downloading ninja-1.10.2.4-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (120 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m120.7/120.7 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<1.25,>=1.19.1 in /usr/local/lib/python3.10/dist-packages (from nncf==2.5.0.dev0+9c30388f) (1.23.5)\n",
            "Collecting openvino-telemetry>=2023.1.1 (from nncf==2.5.0.dev0+9c30388f)\n",
            "  Downloading openvino_telemetry-2023.1.1-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from nncf==2.5.0.dev0+9c30388f) (23.1)\n",
            "Requirement already satisfied: pandas<2.1,>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from nncf==2.5.0.dev0+9c30388f) (1.5.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from nncf==2.5.0.dev0+9c30388f) (5.9.5)\n",
            "Requirement already satisfied: pydot>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nncf==2.5.0.dev0+9c30388f) (1.4.2)\n",
            "Collecting pyparsing<3.0 (from nncf==2.5.0.dev0+9c30388f)\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from nncf==2.5.0.dev0+9c30388f) (1.2.2)\n",
            "Requirement already satisfied: scipy<1.11,>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from nncf==2.5.0.dev0+9c30388f) (1.10.1)\n",
            "Collecting texttable>=1.6.3 (from nncf==2.5.0.dev0+9c30388f)\n",
            "  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: tqdm>=4.54.1 in /usr/local/lib/python3.10/dist-packages (from nncf==2.5.0.dev0+9c30388f) (4.66.1)\n",
            "Requirement already satisfied: torch<2.1,>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from nncf==2.5.0.dev0+9c30388f) (2.0.1+cu118)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2.0->nncf==2.5.0.dev0+9c30388f) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2.0->nncf==2.5.0.dev0+9c30388f) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2.0->nncf==2.5.0.dev0+9c30388f) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2.0->nncf==2.5.0.dev0+9c30388f) (0.9.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.1,>=1.1.5->nncf==2.5.0.dev0+9c30388f) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.1,>=1.1.5->nncf==2.5.0.dev0+9c30388f) (2023.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.0->nncf==2.5.0.dev0+9c30388f) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.0->nncf==2.5.0.dev0+9c30388f) (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=1.13.0->nncf==2.5.0.dev0+9c30388f) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=1.13.0->nncf==2.5.0.dev0+9c30388f) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=1.13.0->nncf==2.5.0.dev0+9c30388f) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=1.13.0->nncf==2.5.0.dev0+9c30388f) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=1.13.0->nncf==2.5.0.dev0+9c30388f) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<2.1,>=1.13.0->nncf==2.5.0.dev0+9c30388f) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<2.1,>=1.13.0->nncf==2.5.0.dev0+9c30388f) (16.0.6)\n",
            "Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.10/dist-packages (from pymoo@ git+https://github.com/anyoptimization/pymoo.git@695cb26923903f872c7256a9013609769f3cc2bd->nncf==2.5.0.dev0+9c30388f) (3.7.1)\n",
            "Requirement already satisfied: autograd>=1.4 in /usr/local/lib/python3.10/dist-packages (from pymoo@ git+https://github.com/anyoptimization/pymoo.git@695cb26923903f872c7256a9013609769f3cc2bd->nncf==2.5.0.dev0+9c30388f) (1.6.2)\n",
            "Collecting cma==3.2.2 (from pymoo@ git+https://github.com/anyoptimization/pymoo.git@695cb26923903f872c7256a9013609769f3cc2bd->nncf==2.5.0.dev0+9c30388f)\n",
            "  Downloading cma-3.2.2-py2.py3-none-any.whl (249 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m249.1/249.1 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alive-progress (from pymoo@ git+https://github.com/anyoptimization/pymoo.git@695cb26923903f872c7256a9013609769f3cc2bd->nncf==2.5.0.dev0+9c30388f)\n",
            "  Downloading alive_progress-3.1.4-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill (from pymoo@ git+https://github.com/anyoptimization/pymoo.git@695cb26923903f872c7256a9013609769f3cc2bd->nncf==2.5.0.dev0+9c30388f)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Deprecated (from pymoo@ git+https://github.com/anyoptimization/pymoo.git@695cb26923903f872c7256a9013609769f3cc2bd->nncf==2.5.0.dev0+9c30388f)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd>=1.4->pymoo@ git+https://github.com/anyoptimization/pymoo.git@695cb26923903f872c7256a9013609769f3cc2bd->nncf==2.5.0.dev0+9c30388f) (0.18.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo@ git+https://github.com/anyoptimization/pymoo.git@695cb26923903f872c7256a9013609769f3cc2bd->nncf==2.5.0.dev0+9c30388f) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo@ git+https://github.com/anyoptimization/pymoo.git@695cb26923903f872c7256a9013609769f3cc2bd->nncf==2.5.0.dev0+9c30388f) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo@ git+https://github.com/anyoptimization/pymoo.git@695cb26923903f872c7256a9013609769f3cc2bd->nncf==2.5.0.dev0+9c30388f) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo@ git+https://github.com/anyoptimization/pymoo.git@695cb26923903f872c7256a9013609769f3cc2bd->nncf==2.5.0.dev0+9c30388f) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo@ git+https://github.com/anyoptimization/pymoo.git@695cb26923903f872c7256a9013609769f3cc2bd->nncf==2.5.0.dev0+9c30388f) (9.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas<2.1,>=1.1.5->nncf==2.5.0.dev0+9c30388f) (1.16.0)\n",
            "Collecting about-time==4.2.1 (from alive-progress->pymoo@ git+https://github.com/anyoptimization/pymoo.git@695cb26923903f872c7256a9013609769f3cc2bd->nncf==2.5.0.dev0+9c30388f)\n",
            "  Downloading about_time-4.2.1-py3-none-any.whl (13 kB)\n",
            "Collecting grapheme==0.6.0 (from alive-progress->pymoo@ git+https://github.com/anyoptimization/pymoo.git@695cb26923903f872c7256a9013609769f3cc2bd->nncf==2.5.0.dev0+9c30388f)\n",
            "  Downloading grapheme-0.6.0.tar.gz (207 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated->pymoo@ git+https://github.com/anyoptimization/pymoo.git@695cb26923903f872c7256a9013609769f3cc2bd->nncf==2.5.0.dev0+9c30388f) (1.14.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.1,>=1.13.0->nncf==2.5.0.dev0+9c30388f) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.1,>=1.13.0->nncf==2.5.0.dev0+9c30388f) (1.3.0)\n",
            "Building wheels for collected packages: jstyleson, nncf, pymoo, grapheme\n",
            "  Building wheel for jstyleson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jstyleson: filename=jstyleson-0.0.2-py3-none-any.whl size=2384 sha256=731c8f5d4e87d4b4b37c2c6e764851e58559577e373cdd7ca490376de7c59072\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/51/c6/a1e751db88203e11c6d9ffe4683ca3d8c14b1479639bec1006\n",
            "  Building wheel for nncf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nncf: filename=nncf-2.5.0.dev0+9c30388f-py3-none-any.whl size=1139920 sha256=11a9ec27c986971b284afe501015fe351dbeab9dedd2788560bfddc44046ed39\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xho9uta8/wheels/dd/e6/7b/14fe3a58e08fb43dadb909606dbb961f65984f671c4e72abb0\n",
            "  Building wheel for pymoo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pymoo: filename=pymoo-0.6.0.1-cp310-cp310-linux_x86_64.whl size=3225993 sha256=5eb6e5e256d054b4467fe46879cab5790055ab59592a99d8194ca2e78d9db159\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/76/84/56f244b9306940b915c56642523019ec6846a50b9aca36076d\n",
            "  Building wheel for grapheme (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grapheme: filename=grapheme-0.6.0-py3-none-any.whl size=210079 sha256=2b8e2af51c47ac28afb30241a254a521ce7cd5865ac4188e3457d7bc8794fa93\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/e1/49/37e6bde9886439057450c494a79b0bef8bbe897a54aebfc757\n",
            "Successfully built jstyleson nncf pymoo grapheme\n",
            "Installing collected packages: texttable, openvino-telemetry, ninja, jstyleson, grapheme, pyparsing, networkx, dill, Deprecated, cma, about-time, alive-progress, pymoo, nncf\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.1.1\n",
            "    Uninstalling pyparsing-3.1.1:\n",
            "      Successfully uninstalled pyparsing-3.1.1\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.1\n",
            "    Uninstalling networkx-3.1:\n",
            "      Successfully uninstalled networkx-3.1\n",
            "Successfully installed Deprecated-1.2.14 about-time-4.2.1 alive-progress-3.1.4 cma-3.2.2 dill-0.3.7 grapheme-0.6.0 jstyleson-0.0.2 networkx-2.8.2 ninja-1.10.2.4 nncf-2.5.0.dev0+9c30388f openvino-telemetry-2023.1.1 pymoo-0.6.0.1 pyparsing-2.4.7 texttable-1.6.7\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pyparsing"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "# %%capture\n",
        "%cd nncf\n",
        "!pip install .[torch]\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwLu99qZ3IEl",
        "outputId": "c017febe-b1b2-4e43-9ec1-d4a520c7ffd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, tensorflow\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import nncf.torch  # Important - must be imported before any other external package that depends on torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwXZ_Te7El1C"
      },
      "source": [
        "https://github.com/openvinotoolkit/nncf/tree/develop\n",
        "https://github.com/openvinotoolkit/nncf/blob/develop/docs/compression_algorithms/Quantization.md\n",
        "https://github.com/openvinotoolkit/nncf/blob/develop/examples/torch/classification/configs/quantization/inception_v3_imagenet_int8.json\n",
        "\n",
        "https://dev-discuss.pytorch.org/t/torch-ao-quantization-migration-plan/2810\n",
        "https://docs.pytorch.org/ao/stable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ysq3_CdFoNuu"
      },
      "source": [
        "** **:  Post Training Quantization   float32 torch2     .        .          (.  ).         SENSITIVITY .      .     / ."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "snxY8jLOD-OC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YWFXWLjp6SQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.quantization\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import copy\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Q8bMWILT6SS9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "backend = 'fbgemm'\n",
        "torch.backends.quantized.engine = backend"
      ],
      "metadata": {
        "id": "GlI97oE46SXo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet18(pretrained=True)\n",
        "model.fc = nn.Linear(model.fc.in_features, 10)"
      ],
      "metadata": {
        "id": "qPDVACQc6SdU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.Resize(224), transforms.ToTensor(), transforms.Normalize((0.5,)*3, (0.5,)*3)])\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "calib_loader = torch.utils.data.DataLoader(torch.utils.data.Subset(trainset, range(500)), batch_size=64)\n",
        "#testloader = torch.utils.data.DataLoader(testset, batch_size=64)\n",
        "\n",
        "fast_testset = torch.utils.data.Subset(testset, range(1000))\n",
        "testloader = torch.utils.data.DataLoader(fast_testset, batch_size=128)"
      ],
      "metadata": {
        "id": "ZCQZ48bu6SjA"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "def measure_time(model, loader):\n",
        "    model.eval()\n",
        "    start = time.time()\n",
        "    with torch.no_grad():\n",
        "        for images, _ in loader:\n",
        "            model(images)\n",
        "    return (time.time() - start) * 1000 / len(loader)"
      ],
      "metadata": {
        "id": "7Eu8lUnX6Sof"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fp32 = copy.deepcopy(model)\n",
        "print(\"copy\")\n",
        "acc_fp32 = evaluate(model_fp32, testloader)\n",
        "print(\"acc\")\n",
        "time_fp32 = measure_time(model_fp32, testloader)\n",
        "print(\"time\")\n",
        "\n",
        "print(acc_fp32)\n",
        "print(time_fp32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9C3eRljT6Sux",
        "outputId": "702f7bcb-86cc-4cfc-cd83-16c980ea982d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "copy\n",
            "acc\n",
            "time\n",
            "10.6\n",
            "12726.011395454407\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = copy.deepcopy(model)\n",
        "m.eval()\n",
        "qconfig_dict = {\"\": torch.quantization.get_default_qconfig(backend)}\n",
        "example_inputs = (next(iter(calib_loader))[0],)\n",
        "model_prepared = quantize_fx.prepare_fx(m, qconfig_dict, example_inputs)\n",
        "with torch.no_grad():\n",
        "    for data in calib_loader:\n",
        "        model_prepared(data[0])\n",
        "model_quant = quantize_fx.convert_fx(model_prepared)\n",
        "acc_quant = evaluate(model_quant, testloader)\n",
        "time_quant = measure_time(model_quant, testloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xr6tWaSI6S1l",
        "outputId": "bcdbaecb-22be-4514-d161-2f9fa2278e09"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3049405352.py:5: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model_prepared = quantize_fx.prepare_fx(m, qconfig_dict, example_inputs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  prepared = prepare(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-3049405352.py:9: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model_quant = quantize_fx.convert_fx(model_prepared)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qconfig1 = torch.quantization.QConfig(activation=torch.quantization.MinMaxObserver, weight=torch.quantization.MinMaxObserver)\n",
        "qconfig_dict1 = {\"\": qconfig1}\n",
        "m1 = copy.deepcopy(model)\n",
        "m1.eval()\n",
        "prep1 = quantize_fx.prepare_fx(m1, qconfig_dict1, example_inputs)\n",
        "with torch.no_grad():\n",
        "    for data in calib_loader:\n",
        "        prep1(data[0])\n",
        "quant1 = quantize_fx.convert_fx(prep1)\n",
        "acc1 = evaluate(quant1, testloader)\n",
        "time1 = measure_time(quant1, testloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXSqxKQc6S7J",
        "outputId": "984fa87e-fb61-48a6-b032-bcea0538186d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-553867197.py:5: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  prep1 = quantize_fx.prepare_fx(m1, qconfig_dict1, example_inputs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  prepared = prepare(\n",
            "/tmp/ipython-input-553867197.py:9: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quant1 = quantize_fx.convert_fx(prep1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qconfig2 = torch.quantization.QConfig(activation=torch.quantization.HistogramObserver, weight=torch.quantization.HistogramObserver)\n",
        "qconfig_dict2 = {\"\": qconfig2}\n",
        "m2 = copy.deepcopy(model)\n",
        "m2.eval()\n",
        "prep2 = quantize_fx.prepare_fx(m2, qconfig_dict2, example_inputs)\n",
        "with torch.no_grad():\n",
        "    for data in calib_loader:\n",
        "        prep2(data[0])\n",
        "quant2 = quantize_fx.convert_fx(prep2)\n",
        "acc2 = evaluate(quant2, testloader)\n",
        "time2 = measure_time(quant2, testloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0a6SGIt6TkR",
        "outputId": "64d36cd5-e367-4be8-806f-8e7ce3633c89"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1289733563.py:5: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  prep2 = quantize_fx.prepare_fx(m2, qconfig_dict2, example_inputs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  prepared = prepare(\n",
            "/tmp/ipython-input-1289733563.py:9: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quant2 = quantize_fx.convert_fx(prep2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qconfig_dict_sens = {\"\": torch.quantization.get_default_qconfig(backend), \"module_name\": [(\"fc\", None)]}\n",
        "m_sens = copy.deepcopy(model)\n",
        "m_sens.eval()\n",
        "prep_sens = quantize_fx.prepare_fx(m_sens, qconfig_dict_sens, example_inputs)\n",
        "with torch.no_grad():\n",
        "    for data in calib_loader:\n",
        "        prep_sens(data[0])\n",
        "quant_sens = quantize_fx.convert_fx(prep_sens)\n",
        "acc_sens = evaluate(quant_sens, testloader)\n",
        "time_sens = measure_time(quant_sens, testloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSy4V1hv6Tnq",
        "outputId": "4897403a-b70b-4296-f70e-648bdff6aa12"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2378824655.py:4: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  prep_sens = quantize_fx.prepare_fx(m_sens, qconfig_dict_sens, example_inputs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  prepared = prepare(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-2378824655.py:8: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quant_sens = quantize_fx.convert_fx(prep_sens)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = [\n",
        "    {'Model': 'FP32', 'Acc': acc_fp32, 'Time': time_fp32},\n",
        "    {'Model': 'Default Quant', 'Acc': acc_quant, 'Time': time_quant},\n",
        "    {'Model': 'MinMax', 'Acc': acc1, 'Time': time1},\n",
        "    {'Model': 'Histogram', 'Acc': acc2, 'Time': time2},\n",
        "    {'Model': 'With Sensitivity (no fc quant)', 'Acc': acc_sens, 'Time': time_sens}\n",
        "]\n",
        "df = pd.DataFrame(results)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "So8HxF1f6Ts9",
        "outputId": "daff8b64-fbd4-466e-a7bd-d642017bd917"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            Model   Acc          Time\n",
            "0                            FP32  10.6  12726.011395\n",
            "1                   Default Quant  10.9   5637.218952\n",
            "2                          MinMax  10.6  11004.799217\n",
            "3                       Histogram  10.5  11115.070730\n",
            "4  With Sensitivity (no fc quant)  10.9   5862.713009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K1cN5HgJ7d6K"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}